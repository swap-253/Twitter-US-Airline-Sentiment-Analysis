{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "INsvANBUQ_Nn"
   },
   "outputs": [],
   "source": [
    "#importing libraries for model evaluation and algorithms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "from sklearn import preprocessing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#dl libraraies\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization,Reshape,Dot,Concatenate,Add,Lambda,Bidirectional\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping\n",
    "import cv2\n",
    "from tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# specifically for deeplearning.\n",
    "from tensorflow.keras.layers import Dropout, Flatten,Activation,Input,Embedding,GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from kerastuner import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "DoU4sypRQ_Gj",
    "outputId": "4120368a-4c18-45a1-e129-999b7979887c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  ...               user_timezone\n",
       "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
       "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
       "2  570301083672813571  ...  Central Time (US & Canada)\n",
       "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
       "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the dataframe\n",
    "df=pd.read_csv('Tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGb7zeFpwSc4",
    "outputId": "c284ed8b-3fa6-4df7-a608-570c0274cc7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9178\n",
       "1    2363\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing the neutral sentiments considering that positive and negative sentiments matter more \n",
    "df=df[df['airline_sentiment']!='neutral']\n",
    "#now since neutral elements are deleted so I need to reset the indices\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "#positive sentiments to 1 and negative to 0 \n",
    "def partition(x):\n",
    "    if x =='positive':\n",
    "        return 1\n",
    "    return 0\n",
    "actualSentiment = df['airline_sentiment']\n",
    "positiveNegative = actualSentiment.map(partition) \n",
    "df['Sentiment'] = positiveNegative\n",
    "df['Sentiment'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "Wfbovcn8wSfu",
    "outputId": "d91d7780-31ad-4db5-94da-909124c93527"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SoDUJUsPU0M3"
   },
   "outputs": [],
   "source": [
    "#Setting parameters which will be used throughout\n",
    "num_words = 15000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "val_size = 1000  # Size of the validation set\n",
    "epochs = 20  # Number of epochs we usually start to train with\n",
    "batch_size = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "#Taking only two columns since it's a sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iJTh5-ShU0P2"
   },
   "outputs": [],
   "source": [
    "#tweets conssits of every document as an array of tokenized words which are later appended to docs \n",
    "tweets=[word_tokenize(tweet) for tweet in df['text']]\n",
    "docs=[]\n",
    "for j in range(0,len(tweets)):\n",
    "    docs.append(tweets[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "F-rXkF_1U0iE"
   },
   "outputs": [],
   "source": [
    "#stops included both the stopwords and punctuations\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stops = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "not_list = [\"n't\", \"not\", \"no\"]\n",
    "stops.update(punctuations)\n",
    "stops.update(not_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1mxGxUnBWSv3"
   },
   "outputs": [],
   "source": [
    "#to get the simple pos(part of speech) tag\n",
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_nO4zHxZWSzJ"
   },
   "outputs": [],
   "source": [
    "#to get the pos tag for a word\n",
    "from nltk import pos_tag\n",
    "# now we are going to clean our data \n",
    "# we will remove stopwords and punctuations and lemmatize each document\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def clean(words):\n",
    "    output=[]\n",
    "    for word in words:\n",
    "        if word.lower() not in stops or word.lower() in not_list:\n",
    "            pos=pos_tag(word)\n",
    "            clean_word=lemmatizer.lemmatize(word,pos=get_simple_pos(pos[0][1]))\n",
    "            output.append(clean_word.lower())\n",
    "    str1=\" \".join(output).encode('utf-8')        \n",
    "    return str1\n",
    "docs=[ clean(doc) for doc in docs]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "HEdvYfQdWS1m",
    "outputId": "9f4efe8a-9c0e-451f-e3bc-c5f64340c029"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica plus 've added commercial experi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica plus 've added commercial experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica 's really aggressive blast obnox...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica 's really aggressive blast obnox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica 's really big bad thing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica 's really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570300767074181121</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0.6842</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica seriously would pay 30 flight se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:33 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica seriously would pay 30 flight se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300616901320704</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cjmcginnis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica yes nearly every time fly vx “ e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:13:57 -0800</td>\n",
       "      <td>San Francisco CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica yes nearly every time fly vx “ e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  ...                                        CleanedText\n",
       "0  570301130888122368  ...  virginamerica plus 've added commercial experi...\n",
       "1  570301031407624196  ...  virginamerica 's really aggressive blast obnox...\n",
       "2  570300817074462722  ...              virginamerica 's really big bad thing\n",
       "3  570300767074181121  ...  virginamerica seriously would pay 30 flight se...\n",
       "4  570300616901320704  ...  virginamerica yes nearly every time fly vx “ e...\n",
       "\n",
       "[5 rows x 17 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#docs was the cleaned lemmatized text which has been appended into the dataframe for further use\n",
    "df['CleanedText']=docs\n",
    "df['CleanedText']=df['CleanedText'].str.decode(\"utf-8\")\n",
    "def remove_mentions(input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "df.text = df.CleanedText.apply(remove_mentions)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "eywgfErHwShp",
    "outputId": "b8194f3e-c005-4b6b-94cf-d13d086bc562"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>virginamerica plus 've added commercial experi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginamerica 's really aggressive blast obnox...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virginamerica 's really big bad thing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>virginamerica seriously would pay 30 flight se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virginamerica yes nearly every time fly vx “ e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         CleanedText  Sentiment\n",
       "0  virginamerica plus 've added commercial experi...          1\n",
       "1  virginamerica 's really aggressive blast obnox...          0\n",
       "2              virginamerica 's really big bad thing          0\n",
       "3  virginamerica seriously would pay 30 flight se...          0\n",
       "4  virginamerica yes nearly every time fly vx “ e...          1"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking only two columns in the dataframe\n",
    "df=df[['CleanedText','Sentiment']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_B9KW6XjAVa_"
   },
   "outputs": [],
   "source": [
    "#taking variables to be used for train test split as X,y\n",
    "X,Y=df['CleanedText'].values,pd.get_dummies(df.Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e57a2yp9AVfN",
    "outputId": "b8578567-953d-462c-a7ce-30ad3e9d5464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted tokenizer on 11541 documents\n",
      "15000 words in dictionary\n",
      "Top 5 most common words are: [('united', 3426), ('flight', 3321), ('usairways', 2651), ('americanair', 2465), (\"n't\", 1878)]\n"
     ]
    }
   ],
   "source": [
    "#using tokenizers to create the tokens having no of words=15000(num_words)\n",
    "tk = Tokenizer(num_words=num_words,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "#Complete data is tokenized to vectors and padding is done using zeros to match its length to the largest text in the dataset.\n",
    "tk.fit_on_texts(X)\n",
    "X = tk.texts_to_sequences(X)\n",
    "X = pad_sequences(X)\n",
    "#print(X[:2])\n",
    "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
    "print('{} words in dictionary'.format(tk.num_words))\n",
    "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CeCPtjnUWIl",
    "outputId": "d71bf2c9-30dd-4162-8b27-fc4eed9d34f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data samples: (9232, 27)\n",
      "# Test data samples: (2309, 27)\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "print('# Train data samples:', X_train.shape)\n",
    "print('# Test data samples:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hndF1pitCISi",
    "outputId": "dcab6cdc-ba07-4b7b-b99f-480183ae4229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (924, 27)\n"
     ]
    }
   ],
   "source": [
    "#getting validation data as a part of training data\n",
    "X_train_rest, X_valid, Y_train_rest, Y_valid = train_test_split(X_train,Y_train, test_size=0.1, random_state=37)\n",
    "print('Shape of validation set:',X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "EnlWWTk-E4D4"
   },
   "outputs": [],
   "source": [
    "#Function defined to test the models in the test set\n",
    "def test_model(model, epoch_stop):\n",
    "    model.fit(X_test,Y_test,epochs=epoch_stop,batch_size=batch_size,verbose=0)\n",
    "    results = model.evaluate(X_test, Y_test)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1kMJUhJOnDL",
    "outputId": "ba27232b-dadb-4315-e0c4-d1df6af87d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 27, 128)           1920000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 27, 392)           509600    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 27, 128)           233984    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 2,668,274\n",
      "Trainable params: 2,668,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128 #dimension of the word embedding vector for each word in a sequence \n",
    "lstm_out = 196  #no of lstm layers\n",
    "bidir_lstm_model = Sequential()\n",
    "bidir_lstm_model.add(Embedding(num_words, embed_dim,input_length = X_train.shape[1]))\n",
    "bidir_lstm_model.add(Bidirectional(LSTM(lstm_out, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))) \n",
    "bidir_lstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "bidir_lstm_model.add(GlobalMaxPooling1D()) #Pooling Layer decreases sensitivity to features, thereby creating more \n",
    "#generalised data for better test results.\n",
    "#Adding a regularized dense layer\n",
    "bidir_lstm_model.add(layers.Dense(32,kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n",
    "bidir_lstm_model.add(layers.Dropout(0.25))\n",
    "bidir_lstm_model.add(layers.Dense(16,activation='relu'))\n",
    "bidir_lstm_model.add(layers.Dropout(0.25))\n",
    "bidir_lstm_model.add(Dense(2,activation='softmax'))\n",
    "bidir_lstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(bidir_lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ZBKWK7e5LreY"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly\n",
    "        monitor='val_accuracy',\n",
    "        min_delta=1e-4,\n",
    "        patience=4,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='weights.h5',\n",
    "        monitor='val_accuracy', \n",
    "        mode='max', \n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnaNvVukOnIx",
    "outputId": "f78f9a76-f2e8-4dc4-ad09-a66dc4c95766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "17/17 [==============================] - 48s 2s/step - loss: 0.6701 - accuracy: 0.7589 - val_loss: 0.5476 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79545, saving model to weights.h5\n",
      "Epoch 2/4\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.5526 - accuracy: 0.7897 - val_loss: 0.3648 - val_accuracy: 0.8647\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.79545 to 0.86472, saving model to weights.h5\n",
      "Epoch 3/4\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.3450 - accuracy: 0.8604 - val_loss: 0.2644 - val_accuracy: 0.9253\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.86472 to 0.92532, saving model to weights.h5\n",
      "Epoch 4/4\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.2145 - accuracy: 0.9399 - val_loss: 0.2675 - val_accuracy: 0.9264\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.92532 to 0.92641, saving model to weights.h5\n"
     ]
    }
   ],
   "source": [
    "#model trained on the training data and taking validation data into account to avoid overfitting for 4 epochs \n",
    "history_LSTM=bidir_lstm_model.fit(X_train_rest, Y_train_rest, epochs = 4, batch_size=batch_size,validation_data=(X_valid, Y_valid),verbose = 1,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgOA7WOIeQSZ",
    "outputId": "e53e82e3-dbe4-4507-a885-e6fab6a889f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 5s 62ms/step - loss: 0.1276 - accuracy: 0.9710\n",
      "/n\n",
      "Test accuracy of lstm model: 97.10%\n"
     ]
    }
   ],
   "source": [
    "#prediction by our bidirectional model on the test dataset\n",
    "bidir_lstm_results = test_model(bidir_lstm_model, 4)\n",
    "print('/n')\n",
    "print('Test accuracy of lstm model: {0:.2f}%'.format(bidir_lstm_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "D48NAU0nXEcR"
   },
   "outputs": [],
   "source": [
    "#building the model using Keras Tuner so taking a range of values for the layers desired to be tuned.\n",
    "def build_model(hp):  \n",
    "    tuned_lstm_model = keras.Sequential([     \n",
    "     # giving range of values for output_dim for tuning for Embedding layer                                  \n",
    "     keras.layers.Embedding(\n",
    "        input_dim=num_words, \n",
    "        output_dim=hp.Int('embeddings', min_value=32, max_value=128, step=16),\n",
    "        input_length = X_train.shape[1]\n",
    "    ),      \n",
    "      # giving range of values for no. of units for tuning for LSTM layer\n",
    "      keras.layers.Bidirectional(LSTM(\n",
    "        units=hp.Int('lstm_1_units', min_value=64, max_value=224,step=32), \n",
    "        dropout=hp.Int('drop_1_units', min_value=0, max_value=8,step=2)/10,\n",
    "        recurrent_dropout=0.0\n",
    "    )),                                                  \n",
    "    # giving range of values for no. of units for tuning for dense layer\n",
    "    keras.layers.Dense(\n",
    "        units=hp.Int('dense_2_units', min_value=32, max_value=128, step=16),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.001)\n",
    "    ),   \n",
    "    # giving range of values for rate for tuning for dropout layer\n",
    "    keras.layers.Dropout(\n",
    "        rate=hp.Int('drop_1_rate', min_value=1, max_value=8, step=1)/10), \n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "  ])\n",
    "  #model compiled using Adam's optimizer with choices for learning rate\n",
    "  tuned_lstm_model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "  \n",
    "  return tuned_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Ww9khaEBYfK9"
   },
   "outputs": [],
   "source": [
    "#A function defined for keras tuner to look at each of the models\n",
    "tuner_search=RandomSearch(build_model,\n",
    "                          objective='val_accuracy',\n",
    "                          max_trials=5,directory='dir',\n",
    "    project_name='lstm_sentiment_Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yvETFy5PYfP1",
    "outputId": "dbe58983-c001-4f38-b4bd-1901303c06f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 02m 26s]\n",
      "val_accuracy: 0.9134199023246765\n",
      "\n",
      "Best val_accuracy So Far: 0.9372294545173645\n",
      "Total elapsed time: 00h 08m 27s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#function called to search the results for different combinations of the parameter values in layers\n",
    "tuner_search.search(X_train_rest,Y_train_rest,epochs=3,validation_data=(X_valid,Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6YWjAFGYfRh",
    "outputId": "cd1bc62a-6716-4454-8f48-2ba7d27ef346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 27, 96)            1440000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 320)               328960    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 80)                25680     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 162       \n",
      "=================================================================\n",
      "Total params: 1,794,802\n",
      "Trainable params: 1,794,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#to get the best model parameters\n",
    "tuned_bidir_lstm_model=tuner_search.get_best_models(num_models=1)[0]\n",
    "tuned_bidir_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9m_m1UbQdXTt",
    "outputId": "bf45effa-1bc1-4ea5-9771-4ecc9a1e4ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "17/17 [==============================] - 17s 792ms/step - loss: 0.1548 - accuracy: 0.9573 - val_loss: 0.1925 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.92641 to 0.92857, saving model to weights.h5\n",
      "Epoch 2/4\n",
      "17/17 [==============================] - 13s 741ms/step - loss: 0.1298 - accuracy: 0.9583 - val_loss: 0.1885 - val_accuracy: 0.9426\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.92857 to 0.94264, saving model to weights.h5\n",
      "Epoch 3/4\n",
      "17/17 [==============================] - 13s 739ms/step - loss: 0.1091 - accuracy: 0.9691 - val_loss: 0.1896 - val_accuracy: 0.9372\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.94264\n",
      "Epoch 4/4\n",
      "17/17 [==============================] - 13s 737ms/step - loss: 0.0886 - accuracy: 0.9764 - val_loss: 0.1915 - val_accuracy: 0.9394\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.94264\n"
     ]
    }
   ],
   "source": [
    "#model trained on the training data and taking validation data into account to avoid overfitting for 4 epochs \n",
    "history_LSTM=tuned_bidir_lstm_model.fit(X_train_rest, Y_train_rest, epochs = 4, batch_size=batch_size,validation_data=(X_valid, Y_valid),\n",
    "                                        verbose = 1,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ed-W3dZ4dBd3",
    "outputId": "eb992ad7-3c86-49c8-f0d4-20c3df2c77e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 2s 31ms/step - loss: 0.1068 - accuracy: 0.9723\n",
      "/n\n",
      "Test accuracy of lstm model: 97.23%\n"
     ]
    }
   ],
   "source": [
    "#prediction by our tuned bidirectional lstm model on the test dataset. This accuracy is better than the untuned model\n",
    "lstm_results = test_model(tuned_bidir_lstm_model,4)\n",
    "print('/n')\n",
    "print('Test accuracy of lstm model: {0:.2f}%'.format(lstm_results[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kk26OATDiXXv"
   },
   "source": [
    "##Now we will see the predictions by our model on sample movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGinF3USilsL"
   },
   "source": [
    "***Class 1 is positive class and class 0 is negative class***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQwynIG4HyeL",
    "outputId": "2e99bdab-760e-4889-b9aa-add4810139dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First the sample text is a positive review, so lets see its prediction by our model\n",
    "sample_text = ('The movie was good. Everything was so awesome. Highly recommended.')\n",
    "#converting into sequence because my model is trained on such a sequence\n",
    "sample = tk.texts_to_sequences([sample_text])\n",
    "sample = pad_sequences(sample)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbjpJ6yCZ5mu",
    "outputId": "3f90e7d8-5bd4-4bd1-82e3-1c8623780043"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0., 2423.,\n",
       "        1348., 7534.,   89.,  456., 7534., 2175.,  190., 2243., 3829.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In training 27 was the no of tokens for each tweet so padding by zeros so that model can accept it\n",
    "k=np.zeros((1,27))\n",
    "k[0,-10:]=sample\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8MIEpNnHygz",
    "outputId": "23ac06b9-5b71-4ae2-c541-fbd858040865"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0.04277921 0.9572208 ]\n"
     ]
    }
   ],
   "source": [
    "#Prediction by model and as expected we get the class as 1 by almost 95.7%\n",
    "predictions = bidir_lstm_model.predict_classes(np.array(k))\n",
    "prediction_probability=bidir_lstm_model.predict(np.array(k))\n",
    "print(predictions)\n",
    "print(prediction_probability[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NiqGImrlevsV",
    "outputId": "16510f66-5ded-47b8-97df-3f5440cd7931"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0.06141684 0.9385832 ]\n"
     ]
    }
   ],
   "source": [
    "#Prediction by tuned model and as expected we get the class as 1 by almost 94%\n",
    "predictions = tuned_bidir_lstm_model.predict_classes(np.array(k))\n",
    "prediction_probability=tuned_bidir_lstm_model.predict(np.array(k))\n",
    "print(predictions)\n",
    "print(prediction_probability[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRO6SxWxQ8l3",
    "outputId": "3f00550f-8a21-4fc7-999f-0a02c5bad98b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Secondly the sample text is a negative review, so lets see its prediction by our model\n",
    "sample_text = ('The movie was pathetic and terrible.Nothing was making any sense. Highly unacceptable.')\n",
    "#converting into sequence because my model is trained on such a sequence\n",
    "sample = tk.texts_to_sequences([sample_text])\n",
    "sample = pad_sequences(sample)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0M84WOeQOS4j",
    "outputId": "06844571-aeea-4393-e273-2324063fa91a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,  2423.,  1348.,\n",
       "         7534.,   845.,  1778.,   223.,   153.,  7534.,   228., 12070.,\n",
       "          656.,  2243.,   257.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In training 27 was the no of tokens for each tweet so padding by zeros so that model can accept it\n",
    "k=np.zeros((1,27))\n",
    "k[0,-13:]=sample\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ddzv2kjfj-kf",
    "outputId": "e2cf703b-1ed9-4d00-a2cc-fa35c24f4d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[9.992605e-01 7.394513e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "#Prediction by model and as expected we get the class as 0 by 99.92% probability. \n",
    "predictions = bidir_lstm_model.predict_classes(np.array(k))\n",
    "prediction_probability=bidir_lstm_model.predict(np.array(k))\n",
    "print(predictions)\n",
    "print(prediction_probability[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGejIeFyfDuK",
    "outputId": "f601abb7-d75e-4cfc-f88b-4849c66b6e0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[9.9953699e-01 4.6303307e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "#Prediction by tuned model and as expected we get the class as 0 by 99.95% probability. \n",
    "predictions = tuned_bidir_lstm_model.predict_classes(np.array(k))\n",
    "prediction_probability=tuned_bidir_lstm_model.predict(np.array(k))\n",
    "print(predictions)\n",
    "print(prediction_probability[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Airline Sentiment Analysis Using Bidirectional-LSTMs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
